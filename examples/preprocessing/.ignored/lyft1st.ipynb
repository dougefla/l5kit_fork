{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ecbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1955506d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l5kit version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "print(\"l5kit version:\", l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48822b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9288cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset utils ---\n",
    "from typing import Callable\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset, transform: Callable):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.dataset[index]\n",
    "        return self.transform(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e090ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function utils ---\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\" #torch.Size([32, 1, 303])\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1) # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error).requires_grad_(True)  \n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    \n",
    "    batch_size, _, _, _ = pred.shape\n",
    "    \n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    \n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred, confidences, avails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08430b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "import timm\n",
    "from timm.models.layers.conv2d_same import Conv2dSame\n",
    "\n",
    "\n",
    "class LMM(nn.Module):\n",
    "    def __init__(self, model_architecture, History=30, gem=False):\n",
    "        super().__init__()\n",
    "        self.H = History # 过去3s作为输入帧\n",
    "        num_history_channels = (self.H + 1) * 2 # 62 (过去3s + 当前帧)*2维XY\n",
    "        rgb_channels = 3\n",
    "        num_in_channels = rgb_channels + num_history_channels # 3 + 62\n",
    "\n",
    "        # self.num_modes = 3 # 三条轨迹 \n",
    "        self.num_modes = 1 # 一条轨迹 \n",
    "        \n",
    "        self.future_len = 50 # 输出未来5s\n",
    "        num_targets = 2 * self.future_len #? *2表示仅有XY二维 100\n",
    "        self.num_preds = num_targets * self.num_modes # 轨迹数*轨迹长度*轨迹维度 (2*50)*3\n",
    "\n",
    "        # timm库提取现有backbone: EfficientNetB3\n",
    "        self.backbone = timm.create_model(model_architecture, pretrained=False) \n",
    "\n",
    "#         if gem:\n",
    "#             self.backbone.global_pool = GeM()\n",
    "\n",
    "        self.backbone.conv_stem = Conv2dSame(\n",
    "            num_in_channels, \n",
    "            self.backbone.conv_stem.out_channels,\n",
    "            kernel_size=self.backbone.conv_stem.kernel_size,\n",
    "            stride=self.backbone.conv_stem.stride, # 滑动步长\n",
    "            padding=self.backbone.conv_stem.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.backbone_out_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Identity(),\n",
    "            nn.Linear( # 全连接层\n",
    "                in_features=self.backbone.classifier.in_features,\n",
    "                out_features=self.backbone_out_features,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.lin_head = nn.Sequential( # 由于输入的顺序与构造的结果相关。所以注意邻近层输入输出的size大小\n",
    "            nn.ReLU(),\n",
    "            nn.Linear( # 全连接层 input: [batch_size, input_size]-> output: [batch_size, output_size]\n",
    "                       # 全连接层起到一个矩阵乘法的作用：FC:[input_size, output_size]\n",
    "                       # 输入输出都必须为二维张量，通过.view()来变换\n",
    "                in_features=self.backbone_out_features,\n",
    "#                 out_features=self.num_preds + self.num_modes, # 轨迹数*轨迹长度*轨迹维度 (2*50)*3 + num_modes 3\n",
    "                out_features=self.num_preds,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, image_box,image_sem):\n",
    "        x = torch.cat((image_box,image_sem),dim =1)\n",
    "        x = self.backbone(x)\n",
    "        x = self.lin_head(x)\n",
    "        x = x.view(-1,self.num_modes,self.future_len,2)        \n",
    "        \n",
    "#         if self.training:\n",
    "#             loss_nll = pytorch_neg_multi_log_likelihood_single(targets, x, target_availabilities)\n",
    "# #             print(\"training model\")\n",
    "#             return loss_nll\n",
    "#         else:\n",
    "# #             print(\"evaluation model\")\n",
    "#             return x\n",
    "        return x\n",
    "    \n",
    "class LyftMultiRegressor(nn.Module):\n",
    "    \"\"\"Single mode prediction\"\"\"\n",
    "\n",
    "    def __init__(self, predictor):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor\n",
    "\n",
    "    def forward(self,image_box,image_sem,targets,target_availabilities):\n",
    "        pred = self.predictor(image_box,image_sem)\n",
    "#         if self.PARAMS.predict_diffs:\n",
    "#             pred = torch.cumsum(pred, dim=2)\n",
    "#         pred_sum = torch.cumsum(pred, dim=2)\n",
    "    \n",
    "        loss_nll = pytorch_neg_multi_log_likelihood_single(\n",
    "            targets, pred, target_availabilities\n",
    "        )\n",
    "\n",
    "        return loss_nll, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b750c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utils ---\n",
    "import yaml\n",
    "\n",
    "def save_yaml(filepath, content, width=120):\n",
    "    with open(filepath, 'w') as f:\n",
    "        yaml.dump(content, f, width=width)\n",
    "\n",
    "\n",
    "def load_yaml(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content\n",
    "\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "\n",
    "    Refer: https://stackoverflow.com\n",
    "    /questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary/23689767#23689767\n",
    "    \"\"\"  # NOQA\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "391de3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "#         'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 30,\n",
    "#         'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "#         'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'render_ego_history': True,\n",
    "        'step_time': 0.1\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "#         'raster_size': [448, 224],\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'set_origin_to_bottom': True,\n",
    "        'filter_agents_threshold': 0.5,\n",
    "        'disable_traffic_light_faces': False\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/sample.zarr/train.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 6\n",
    "    },\n",
    "\n",
    "    'valid_data_loader': {\n",
    "        'key': 'scenes/sample.zarr/validate.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'max_num_steps': -1,\n",
    "        'checkpoint_every_n_steps': 100000,\n",
    "\n",
    "        'eval_every_n_steps': 100000\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ccceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_dict = {\n",
    "    \"debug\": False,\n",
    "    # --- Data configs ---\n",
    "    \"l5kit_data_folder\": \"./l5kit_data\",\n",
    "    # --- Model configs ---\n",
    "    \"pred_mode\": \"single\",\n",
    "    # --- Training configs ---\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"out_dir\": \"results/multi_train\",\n",
    "    \"epoch\": 2,\n",
    "    \"snapshot_freq\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e3a8c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flags: {'debug': False, 'l5kit_data_folder': './l5kit_data', 'pred_mode': 'single', 'device': 'cuda:0', 'out_dir': 'results/multi_train', 'epoch': 2, 'snapshot_freq': 50}\n"
     ]
    }
   ],
   "source": [
    "flags = DotDict(flags_dict)\n",
    "out_dir = Path(flags.out_dir)\n",
    "os.makedirs(str(out_dir), exist_ok=True)\n",
    "print(f\"flags: {flags_dict}\")\n",
    "\n",
    "save_yaml(out_dir / 'flags.yaml', flags_dict)\n",
    "save_yaml(out_dir / 'cfg.yaml', cfg)\n",
    "# if in debug mode\n",
    "debug = flags.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd9fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from scenes/sample.zarr/train.zarr\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "4039527\n"
     ]
    }
   ],
   "source": [
    "if debug:    \n",
    "    # set env variable for data\n",
    "    l5kit_data_folder = \"./l5kit_data\"\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = l5kit_data_folder\n",
    "    dm = LocalDataManager(None)\n",
    "\n",
    "    print(\"Load dataset...\")\n",
    "    default_test_cfg = {\n",
    "        'key': 'scenes/sample.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "    }\n",
    "    test_cfg = cfg.get(\"test_data_loader\", default_test_cfg)\n",
    "\n",
    "    # Rasterizer\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "    test_path = test_cfg[\"key\"]\n",
    "    print(f\"Loading from {test_path}\")\n",
    "    test_zarr = ChunkedDataset(dm.require(test_path)).open()\n",
    "    print(\"test_zarr\", type(test_zarr))\n",
    "    # test_mask = np.load(f\"{l5kit_data_folder}/scenes/mask.npz\")[\"arr_0\"]\n",
    "    # test_agent_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
    "    test_agent_dataset = AgentDataset(cfg, test_zarr, rasterizer)\n",
    "    test_dataset = test_agent_dataset\n",
    "    if debug:\n",
    "        # Only use 100 dataset for fast check...\n",
    "        test_dataset = Subset(test_dataset, np.arange(100))\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=test_cfg[\"shuffle\"],\n",
    "        batch_size=test_cfg[\"batch_size\"],\n",
    "        num_workers=test_cfg[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    print(test_agent_dataset)\n",
    "    print(\"# AgentDataset test:\", len(test_agent_dataset))\n",
    "    print(\"# ActualDataset test:\", len(test_dataset))\n",
    "else:\n",
    "    # set env variable for data\n",
    "    l5kit_data_folder = \"/home/fla/workspace/l5kit_data\"\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = l5kit_data_folder\n",
    "    dm = LocalDataManager(None)\n",
    "\n",
    "    # Rasterizer\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "    train_cfg = cfg['train_data_loader']\n",
    "    train_path = train_cfg['key']\n",
    "    print(f\"Loading from {train_path}\")\n",
    "    train_zarr = ChunkedDataset(dm.require(train_path)).open()\n",
    "    # print(\"train_path\", type(train_path))\n",
    "\n",
    "    train_dataset = EgoDataset(cfg, train_zarr, rasterizer)\n",
    "    test_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"],\n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(train_dataset)\n",
    "    print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b21b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "tr_it = iter(test_loader)\n",
    "data = next(tr_it)\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff59e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['image'].shape)# torch.Size([8, 65, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695786a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single mode\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(flags.device)\n",
    "\n",
    "if flags.pred_mode == \"multi\":\n",
    "    predictor = LyftMultiModel(cfg)\n",
    "    model = LyftMultiRegressor(predictor)\n",
    "elif flags.pred_mode == \"single\":\n",
    "    print(\"single mode\")\n",
    "    predictor = LMM(\"tf_efficientnet_b3_ns\")\n",
    "    model = LyftMultiRegressor(predictor)\n",
    "else:\n",
    "    raise ValueError(f\"[ERROR] Unexpected value flags.pred_mode={flags.pred_mode}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce4ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analyis\n",
    "\n",
    "# print(type(data['image']))\n",
    "# data['image'].shape # torch.Size([32, 65, 224, 224])\n",
    "\n",
    "# for para in model.named_parameters():\n",
    "#     print(para[0],'\\t',para[1].size())\n",
    "    \n",
    "# for para in predictor.named_parameters():\n",
    "#     print(para[0],'\\t',para[1].size())\n",
    "    \n",
    "# batch size 调大：出现cpu瓶颈，导致GPU Volatile非常小，跳动。等待cpu传输数据，结果计算时间很长 cpu占用不高\n",
    "# batch size 调小：(1),GPU Volatil稳定在13%左右，loss跳动太大，，cpu占用相对较高\n",
    "# shuffle为false：ave loss不下降 设置为True时会快速下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ea2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False\n",
    "load_path = \"./model/lyft1st_single/chk1.pt\"\n",
    "\n",
    "\n",
    "if resume == True:\n",
    "    pt_file = torch.load(load_path, map_location=lambda storage, loc: storage)\n",
    "    saved_model_param = pt_file[\"model_state_dict\"]\n",
    "    optimizer.load_state_dict(pt_file[\"optimizer_state_dict\"])\n",
    "    losses_train = pt_file[\"loss_arr\"]\n",
    "    loss = pt_file[\"loss\"].to(device) # with grad\n",
    "    trained_steps = pt_file['steps']\n",
    "    print(trained_steps)\n",
    "#     trained_steps = 40000\n",
    "    print(loss)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "#     for key in saved_model_param.keys():\n",
    "#         if key in state_dict and (saved_model_param[key].size() == state_dict[key].size()):# 检查完备\n",
    "#             value = saved_model_param[key]\n",
    "# #             print(type(value))\n",
    "# #             print(value,key)\n",
    "# #             value.requires_grad_(True)\n",
    "#             if not isinstance(value, torch.Tensor):\n",
    "#                 value = value.data\n",
    "#             state_dict[key] = value\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cbe7ba5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image_box'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_box\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      2\u001b[0m image_box_ \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_box\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# dataset_img1 = rasterizer.to_rgb(image_box_[0])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.imshow(dataset_img1[::-1])\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_box'"
     ]
    }
   ],
   "source": [
    "print(type(data['image_box']))\n",
    "image_box_ = data['image_box']\n",
    "# dataset_img1 = rasterizer.to_rgb(image_box_[0])\n",
    "# plt.imshow(dataset_img1[::-1])\n",
    "print('image_box_.shape',image_box_.shape)\n",
    "\n",
    "image_box = image_box_.permute(0,2,1,3)\n",
    "print('image_box.shape',image_box.shape)\n",
    "\n",
    "image_sem = data['image_sem']\n",
    "print('image_sem.shape',image_sem.shape)\n",
    "\n",
    "image_cat = torch.cat((image_box,image_sem),dim =1)\n",
    "# dataset_img1 = rasterizer.to_rgb(image_cat[0])\n",
    "# plt.imshow(dataset_img1[::-1])\n",
    "print(image_cat.shape)\n",
    "print(image_cat[0].shape)\n",
    "print(image_cat[0].permute(1,2,0).shape)\n",
    "to_rgb_image = image_box[0].permute(1,2,0)\n",
    "box_image = to_rgb_image[..., :-3]\n",
    "print(to_rgb_image[..., :-3].shape)\n",
    "print(to_rgb_image[..., :-3].shape[-1] // 2)\n",
    "rgb_image = rasterizer.to_rgb(to_rgb_image)\n",
    "plt.imshow(rgb_image[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafabada",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_steps = 0\n",
    "total_steps = math.ceil(len(train_dataset)/train_cfg['batch_size'])\n",
    "with torch.set_grad_enabled(True):\n",
    "    train = True\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    progress_bar = tqdm(test_loader)\n",
    "    losses_train = []\n",
    "    \n",
    "    for i,data in enumerate(progress_bar):\n",
    "#         if i == 30:\n",
    "#             break\n",
    "        # Forward pass\n",
    "        image_box = data['image_box'].to(device).permute(0,2,1,3)# torch.Size([8, 224, 62, 224]) 不对\n",
    "        image_sem = data['image_sem'].to(device)# torch.Size([8, 3, 224, 224])\n",
    "        targets = data['target_positions'].to(device)\n",
    "        target_availabilities = data['target_availabilities'].to(device)\n",
    "\n",
    "        if train:\n",
    "            loss, pred = model(image_box,image_sem,targets,target_availabilities)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item():.5f} loss(avg): {np.mean(losses_train):.5f}\")\n",
    "        else:\n",
    "            pred = predictor(image,targets,target_availabilities).cpu().numpy()\n",
    "            print(pred.shape)\n",
    "\n",
    "        # Backward pass\n",
    "        chk_pts = cfg[\"train_params\"][\"checkpoint_every_n_steps\"]\n",
    "        eval_pts = cfg[\"train_params\"]['eval_every_n_steps']\n",
    "\n",
    "#         for resume\n",
    "        if i in list(range(chk_pts,total_steps,chk_pts)):\n",
    "            model_index = int((i+trained_steps)/chk_pts)\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss_arr': losses_train, \n",
    "                'loss': loss,\n",
    "                'steps': i},\n",
    "                 f\"./model/lyft1st_single/chk{model_index}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f2197",
   "metadata": {},
   "source": [
    "尽量减少输入的通道数\n",
    "数据的离线预处理\n",
    "合理选择num_workers:CPU与io的速度，内存容量，GPU处理速度\n",
    "CUDA-efficient means “no python control flow”：accessing individual values of GPU tensor may get the job done, but the performance will be awful\n",
    "the slow rasterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7055577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_profiling():\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "        input = torch.randn((8, 1, 128, 128)).cuda()\n",
    "        input.requires_grad = True\n",
    "\n",
    "        target = torch.randint(1, (8, 1, 128, 128)).cuda().float()\n",
    "\n",
    "        for i in range(10):\n",
    "            l = loss(input, target)\n",
    "            l.backward()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
